\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{tikz,lipsum,lmodern}
\usepackage[most]{tcolorbox}
\usepackage{amsmath, amsthm, amssymb, bm}
\title{Assignment 1 - Problem 53}
\author{Vishnu Gollamudi}
\date{January 2022}

\begin{document}

\maketitle

Question 53) Suppose (X,Y) follows bivariate normal distribution with means $\mu1  \mu2$, standard deviations $\sigma1$,$\sigma2$ and correlation coefficient $\rho$, where all parameters are un-known. Then, testing Ho: $\sigma1=\sigma2$ is equivalent to testing the independence of  
\\
1.) X and Y \\
2.) X and X-Y \\
3.) X+Y and Y \\
4.) X+Y and X-Y \\

Answer: 4, X+Y and X-Y \\
Solution: \\
Bi-variate random variables are distribution of normal distribution to two coordinates. are said to be bivariate normal or jointly normal, if $aX+bY$ has normal distribution $\forall$ $a,b \in R$.

Random normal vector A =
$\begin{bmatrix}
 X\\
 Y
\end{bmatrix}$ is Bi-variate when it is jointly normal\\
Joint PDF of A is given as\\
\begin{align*}
    f_a(A) &= \frac{1}{(2\pi)} \sqrt{det {C}} \quad exp \left \Bigg \{ \frac{-1}{2}(a-m) ^ T C^ {-1} (a-m) \right \Bigg \} \\
    \textit{Where},
    m &= \begin{bmatrix}
         \mu_x\\
         \mu_y
        \end{bmatrix}\\
    C &= \begin{bmatrix}
         \rho \sigma_{x}^2 & \sigma_{xy}\\
         \rho \sigma_{yx}  & \sigma_{y}^2
        \end{bmatrix}
\end{align*}

We know that
\\
\begin{align}
  \sigma_{[X+Y,Z]} &= \sigma_{[X,Z]} + \sigma_{[Y,Z]} \\
  \sigma_{[X,X]} &= \sigma_{[X]}^2 \\
  \textit{if}\quad \sigma{[X,Y}] &= \sigma{[X]}^2 \quad \textit{then} \quad X=Y
\end{align}
\large (Proofs for above equations are given in appendix)

\begin{enumerate}
\item \large{Testing for the independence of X,Y}
\begin{enumerate}
\item X,Y can be proven as indepndant if we can prove $\sigma_{XY} = 0$
\item Or if joint PDF can be written as product of PDFs of X,Y
\item With given $\sigma_X = \sigma_X$, both of them can not be proved, hence $H_0$ in dependency can not be tested.
\item In NUll hypothesis if we can not prove X,Y are independant then it does not mean they are independant
\end{enumerate}

\item \large{Testing independence of X,X-Y}
\begin{enumerate}
\item \begin{align*}
    \sigma_{[X-Y,X]} &= \sigma_{[X,X]} - \sigma_{[X,Y]}\\
                     &= \sigma_{X}^2 - \sigma_{[X,Y]}\\
    \sigma_{[X-Y,X]} = 0 \implies \sigma_{X}^2 = \sigma_{[X,Y]}
\end{align*}
\item if $\sigma_{X}^2 = \sigma_{[X,Y]}$ then it means $Y=X$ \\
\end{enumerate}
\begin{tcolorbox}
\large \textbf{Argument}: \small $X-Y, X$ can be independent if and only if  $\sigma_{[X-Y,X]} = 0$. But $\sigma_{[X-Y,X]} = 0 \implies Y=X$, hence they are dependant irrespective of $\sigma_X = \sigma_Y$
\end{tcolorbox}

\item \large Testing for independence of X,X+Y
\begin{enumerate}
\item \begin{align*}
   \sigma_{[X+Y,X]} &= \sigma_{[X,X]} + \sigma_{[X,Y]}\\
                    &= \sigma_{X}^2 + \sigma_{[X,Y]}\\
                    \sigma_{[X+Y,X]} = 0 \implies \sigma_{X}^2 = \sigma_{[X,Y]}\\
\end{align*}
\item if $\sigma_{X}^2 = \sigma_{[X,Y]}$ then it means $Y=-X$ which means they are dependant\\
\begin{tcolorbox}
\large \textbf{Argument}: \small $X+Y, X$ can be independent if and only if  $\sigma_{[X-Y,X]} = 0$. But $\sigma_{[X-Y,X]} = 0 \implies Y=-X$, hence they are dependant irrespective of $\sigma_X = \sigma_Y$
\end{tcolorbox}
\end{enumerate}

\item \large{Testing for independence of X+Y,X-Y}
\begin{enumerate}
\item \begin{align*}
& \hspace{5pt} \sigma_{[X+Y, X-Y]}\\
&= \sigma_{[(X+Y), X]}- \sigma_{[(X+Y), Y]} \\
&= \sigma_{[X,X]} + \sigma_{[X,Y]} - \sigma_{[X,X]} - \sigma_{[X,Y]}  \\
&= \sigma_X^2 - \sigma_Y^2
\end{align*}

\item Now testing for $\sigma_1 = \sigma_2 $ $\implies$ $\sigma_{[X+Y,X-Y]}= 0$
\item Hence testing for   $\sigma_1 = \sigma_2 \implies X+Y, X-Y$ are independent.
\end{enumerate}
\end{enumerate}

\large \textbf{Appendix}\\
Co variance is a measure of how much two random variables vary together
\begin{enumerate}
    \item \large $\sigma_{[X+Y,Z]} = \sigma_{[X,Z]} + \sigma_{[Y,Z]}$
    \begin{align*}
        \sigma_{[X+Y,Z]} &= \mathbf{E[((X+Y) - E(X+Y)) (Z - EZ)^T ]}\\
        &= \mathbf{E((X+Y - \mu_{X}-\mu_{Y})  (Z^T -\mu_{Z}^T ))}\\
        &= \mathbf{E(XZ^T-X\mu_z^T + YZ^T - Y\mu_Z^T - \mu_X Z^T+ \mu_X\mu_Z^T-
        \mu_YZ^T+\mu_Y\mu_Z^T)}\\
        &= \mathbf{E((XZ^T-X\mu_Z^T-\mu_XZ^T+\mu_x\mu_z^T) + (YZ^T-Y\mu_Z^T-\mu_YZ^T+\mu_Y\mu_Z^T))}\\
        &= \mathbf{E((X-\mu_X)(Z^T-\mu_Z^T)+ (Y-\mu_Y)(Z^T-\mu_Z^T))}\\
        &= \mathbf{E((X-\mu_X)(Z-\mu_Z)^T) + E((Y-\mu_Y)(Z-\mu_Z)^T)}\\
        &=\mathbf{\sigma_{[X,Z]} + \sigma_{[Y,Z]}}\\
    \end{align*}
    
    \item \large  $ \sigma_{[X,Y]}= \sigma_{[X]}^2$\\
    \begin{align*}
        \sigma_{[X,Y]} &= \mathbf{E[(X-\mu_X) (Y-\mu_Y)^T]},
        \quad \textit{if}\quad Y=X \\
        \sigma_{[X,X]} &= \mathbf{E[(X-\mu_X) (X-\mu_X)^T]}\\
        &=\sigma_X^2
    \end{align*}
    
    \item if $\sigma{[X,Y}] = \sigma{[X]}^2 \quad \textit{then} \quad X=Y$\\
    From 2 it follows that  $\sigma{[X,Y}] = \sigma{[X]}^2$  when X=Y\\
\end{enumerate}
\end{document}
