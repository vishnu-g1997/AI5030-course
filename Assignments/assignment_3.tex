\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage{gensymb}
\singlespacing
\usepackage[cmex10]{amsmath}
\usepackage{caption}

\usepackage{amsthm}

\usepackage{mathrsfs}
\usepackage{txfonts}
\usepackage{stfloats}
\usepackage{bm}
\usepackage{cite}
\usepackage{cases}
\usepackage{subfig}

\usepackage{longtable}
\usepackage{multirow}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{steinmetz}
\usepackage{tikz}
\usepackage{circuitikz}
\usepackage{verbatim}
\usepackage{tfrupee}
\usepackage[breaklinks=true]{hyperref}
\usepackage{graphicx}
\usepackage{tkz-euclide}

\title{Assignment 3 - Problem 56 Dec-2018 Paper}
\author{Vishnu Gollamudi - AI22MTECH02001}
\pagestyle{fancy}
\fancyhf{}
\rfoot{https://github.com/vishnu-g1997/AI5030-course }

\begin{document}
\large \textbf{Assignment 3 - Problem 56 Dec-2018 Paper}\\
\\
Consider a linear model $Y_1$ = $\theta_1 + \theta_2 + \epsilon_i$ for $l = 1,2$ and $Y_1 = \theta_1-\theta_3 + \epsilon$ for l = 3,4, where $\epsilon_i$ are independent with $R(\epsilon_i) = 0$, $Var (\epsilon_i) = \sigma ^{2} > 0$ for i = 1, ... 4, and $\theta_1,... \theta_3 \in R$. Which of the following parametric function is estimable? 
\\
(A) $\theta_1 + \theta_3$ \\
(B) $\theta_2 - \theta_3$  \\
(C) $\theta_2 + \theta_3$ \\
(D) $\theta_2 + \theta_2 + \theta_3$ \\

Answer: C, $\theta_2 + \theta_3$ \\
\section{Solution:} 
\subsection{Parametric estimation using Maximum Likelihood}
Given Mean of the errors in the both parametric functions is 0 and variance is greater than zero. Assuming the error is Gaussian distributed, the deterministic function can be written as follows\\
\begin{center}
    t = y($X,\Theta$) + $\epsilon$\\
\end{center}
Which means imposing a error distribution on the deterministic function, then we can write 
\begin{center}
    $P(\mathbf{t/y(X,\Theta)},\sigma^2) = N(\mathbf{t/y(X,\Theta)},\sigma^2)$\\
\end{center}
    Where $\Theta = {\theta_1, \theta_2 ... \theta_n}$ are the parameters to estimated \\
    $X = {x_1, x_2..., x_n}$ are input data sets\\
    t = ${t_1,t_2...t_n}$ is the output corresponding to $X = {x_1, x_2..., x_n}$\\
This equation can be represented and solved by taking Log on both sides\\
\begin{align*}
    P(\mathbf{t/y(X,\Theta)},\sigma^2) &= \Pi_{n=1}^N{N(t_n/\Theta^T {X_i}, \sigma^2)}\\
    \ln{P(\mathbf{t/y(X,\Theta)},\sigma^2)} &= \Sigma_{n=1}^N \ln{{N(t_n/\Theta^T {X_i}, \sigma^2)}}\\
\end{align*}
Solving equation on RHS without summation. It is a univariate gaussian
\begin{align*}
    \Sigma_{n=1}^{N} N(t_n/\Theta^T {X_i}, \sigma^2) &= \Sigma_{n=1}^{N} \frac{1}{\sqrt{2\pi\sigma^2}} \exp \{{{\frac{-1}{2\sigma^2}(t_n-\Theta^TX_n)^2}}\}\\
     \ln{N(t_n/\Theta^T {X_n}, \sigma^2)} &= - {\frac{1}{2}\ln{\frac{1}{\sigma^2}}} - \frac{1}{2} \ln{2\pi} - \frac{1}{2\sigma^2} \Sigma_{n=1}^{N} {(t_n - \Theta^T X_n)^2}
\end{align*}
To find the minima of the error, we take gradient on both sides, minimising error is equivalent to maximising the $\Theta$\\Setting Gradient of $\nabla\ln{P(\mathbf{t/y(X,\Theta)},\sigma^2)}$ = 0 we get
\begin{align*}
    0 &= \frac{\partial}{\partial \Theta} { \frac{1}{2} \Sigma_{n=1}^{N}  {(t_n - \Theta^T X_n)^2}}\\
    &= \frac{\partial}{\partial \Theta} \Sigma_{n=1}^{N} {(t_n - \Theta^T X_n)}  \frac{\partial}{\partial \Theta} \Theta^T X_n\\
    &=\Sigma_{n=1}^{N} {(t_n - \Theta^T X_n)} {X_n}^T\\
\end{align*}
We know that $\Theta$ is a vector.
\begin{equation*}
    \frac{\partial}{\partial a} {a^TX} = X^T\\
    \textit{where a is a vector}
\end{equation*}
Now,
\begin{align*}
    0 &= \Sigma_{n=1}^{N} {(t_n - \Theta^T X_n)} {X_n}^T\\
      &= \Sigma_{n=1}^{N} (t_n) (X_n)^T - \Theta^T\Sigma_{n=1}^{N} ( X_n){X_n^T}\\
      &= (t_n) (X_n)^T - \Theta^T ( X_n){X_n^T}\\
\end{align*}
{Taking whole transpose and solving it for $\Theta$ we get}\\
\begin{equation}
    \Theta = [X^TX]^{-1}[X^T\Bar{t}]
\end{equation}


\subsection{Evaluating options for ML}
In all the given options ML can be estimated when error associated with the parametric equations can be minimised by applying derivative on the error and equating it to zero.
Given
\begin{align}
     Y_1 = \theta_1 + \theta_2 + \epsilon_i \hspace{5pt} l = 1,2\\
     Y_1 = \theta_1-\theta_3 + \epsilon_j  \hspace{5pt}  l = 3,4
\end{align}
(3) subtracted from (2) gives us
\begin{align*}
     \theta_2 + \theta_3 + \epsilon_i - \epsilon_j = 0\\
     \theta_2 + \theta_3 = \epsilon_j - \epsilon_i
\end{align*}
Parametric function $\theta_2 + \theta_3$ can be estimated by finding ML for (2) and ML for (3) and then subtracting them both. ML can be found from equation (1)\\
For all other Parametric equations error is not known to take a derivative and equate it to zero.\\
Hence the correct answer is C


\end{document}
